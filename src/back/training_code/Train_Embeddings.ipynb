{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2m5MhxTFmfxA"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "import joblib\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm.auto import tqdm\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# === Configuration ===\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "\n",
        "# === Load and process data ===\n",
        "def load_data(filepath=\"dataset.csv\"):\n",
        "    \"\"\"Load and process the disease-symptom dataset\"\"\"\n",
        "    try:\n",
        "        df = pd.read_csv(filepath)\n",
        "    except FileNotFoundError:\n",
        "        # Fallback paths\n",
        "        alternative_paths = [\n",
        "            \"/kaggle/input/disease-symptom-description-dataset/dataset.csv\",\n",
        "            \"disease_symptom_dataset.csv\"\n",
        "        ]\n",
        "\n",
        "        for path in alternative_paths:\n",
        "            try:\n",
        "                df = pd.read_csv(path)\n",
        "                break\n",
        "            except FileNotFoundError:\n",
        "                continue\n",
        "        else:\n",
        "            raise FileNotFoundError(f\"Could not find dataset at {filepath} or alternative locations\")\n",
        "\n",
        "    print(f\"Dataset loaded with shape: {df.shape}\")\n",
        "    return df\n",
        "\n",
        "def prepare_training_data(df):\n",
        "    \"\"\"Extract disease-symptom pairs from dataframe\"\"\"\n",
        "    symptom_cols = [col for col in df.columns if col.startswith(\"Symptom_\")]\n",
        "\n",
        "    # Create both disease→symptom and symptom→disease pairs\n",
        "    pairs = []\n",
        "\n",
        "    # Extract unique diseases and symptoms for encoding\n",
        "    unique_diseases = []\n",
        "    all_symptoms = []\n",
        "\n",
        "    # Create pairs and collect unique values\n",
        "    for _, row in df.iterrows():\n",
        "        disease = str(row[\"Disease\"]).strip()\n",
        "        symptoms = [str(row[col]).strip() for col in symptom_cols\n",
        "                    if pd.notna(row[col]) and str(row[col]).strip().lower() != \"nan\"]\n",
        "\n",
        "        if disease and symptoms:\n",
        "            unique_diseases.append(disease)\n",
        "            all_symptoms.extend(symptoms)\n",
        "\n",
        "            # Disease→Symptom pairs\n",
        "            for symptom in symptoms:\n",
        "                pairs.append((disease, symptom, \"disease_to_symptom\"))\n",
        "\n",
        "            # Symptom→Disease pairs (combine all symptoms)\n",
        "            symptom_text = \", \".join(symptoms)\n",
        "            pairs.append((symptom_text, disease, \"symptoms_to_disease\"))\n",
        "\n",
        "            # Also add individual symptoms to disease pairs for more granular training\n",
        "            for symptom in symptoms:\n",
        "                pairs.append((symptom, disease, \"symptom_to_disease\"))\n",
        "\n",
        "    # Create encoders\n",
        "    disease_encoder = LabelEncoder()\n",
        "    disease_encoder.fit(list(set(unique_diseases)))\n",
        "\n",
        "    symptom_encoder = LabelEncoder()\n",
        "    symptom_encoder.fit(list(set(all_symptoms)))\n",
        "\n",
        "    print(f\"Found {len(pairs)} training pairs\")\n",
        "    print(f\"Unique diseases: {len(disease_encoder.classes_)}\")\n",
        "    print(f\"Unique symptoms: {len(symptom_encoder.classes_)}\")\n",
        "\n",
        "    return pairs, disease_encoder, symptom_encoder\n",
        "\n",
        "# Split train/test\n",
        "def split_data(pairs):\n",
        "    \"\"\"Split data into train/test sets\"\"\"\n",
        "    train_pairs, test_pairs = train_test_split(pairs, test_size=0.2, random_state=42)\n",
        "    return train_pairs, test_pairs\n",
        "\n",
        "# === Embedding Model ===\n",
        "class JointEmbeddingModel(torch.nn.Module):\n",
        "    \"\"\"Joint embedding model for diseases and symptoms\"\"\"\n",
        "    def __init__(self, model_name=\"dmis-lab/biobert-v1.1\", embedding_dim=768):\n",
        "        super(JointEmbeddingModel, self).__init__()\n",
        "        self.bert = BertModel.from_pretrained(model_name)\n",
        "        self.embedding_dim = embedding_dim\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        outputs = self.bert(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask\n",
        "        )\n",
        "        # Use [CLS] token for sentence embedding\n",
        "        embeddings = outputs.last_hidden_state[:, 0]\n",
        "        # Normalize embeddings\n",
        "        return F.normalize(embeddings, p=2, dim=1)\n",
        "\n",
        "# === Dataset ===\n",
        "class ContrastiveDataset(Dataset):\n",
        "    \"\"\"Dataset for contrastive learning with positive/negative pairs\"\"\"\n",
        "    def __init__(self, pairs, tokenizer, max_length=128):\n",
        "        self.pairs = pairs\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.pairs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text1, text2, pair_type = self.pairs[idx]\n",
        "\n",
        "        # Tokenize both texts\n",
        "        encoding1 = self.tokenizer(\n",
        "            text1,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            max_length=self.max_length,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        encoding2 = self.tokenizer(\n",
        "            text2,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            max_length=self.max_length,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        # Create sample with both texts and their encodings\n",
        "        sample = {\n",
        "            \"text1_input_ids\": encoding1[\"input_ids\"].squeeze(0),\n",
        "            \"text1_attention_mask\": encoding1[\"attention_mask\"].squeeze(0),\n",
        "            \"text2_input_ids\": encoding2[\"input_ids\"].squeeze(0),\n",
        "            \"text2_attention_mask\": encoding2[\"attention_mask\"].squeeze(0),\n",
        "            \"pair_type\": pair_type\n",
        "        }\n",
        "\n",
        "        return sample\n",
        "\n",
        "# === Training Functions ===\n",
        "def train_embedding_model(train_pairs, val_pairs, model_save_path=\"./joint_embedding_model\"):\n",
        "    \"\"\"Train the embedding model using contrastive learning\"\"\"\n",
        "    # Initialize model and tokenizer\n",
        "    model_name = \"dmis-lab/biobert-v1.1\"\n",
        "    tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "    embedding_model = JointEmbeddingModel(model_name).to(device)\n",
        "\n",
        "    # Prepare datasets\n",
        "    train_dataset = ContrastiveDataset(train_pairs, tokenizer)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "\n",
        "    # Optimizer\n",
        "    optimizer = torch.optim.AdamW(embedding_model.parameters(), lr=2e-5)\n",
        "\n",
        "    # Training loop\n",
        "    num_epochs = 3\n",
        "    print(f\"Training embedding model for {num_epochs} epochs...\")\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        embedding_model.train()\n",
        "        total_loss = 0\n",
        "\n",
        "        for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Move batch to device\n",
        "            text1_input_ids = batch[\"text1_input_ids\"].to(device)\n",
        "            text1_attention_mask = batch[\"text1_attention_mask\"].to(device)\n",
        "            text2_input_ids = batch[\"text2_input_ids\"].to(device)\n",
        "            text2_attention_mask = batch[\"text2_attention_mask\"].to(device)\n",
        "\n",
        "            # Get embeddings\n",
        "            text1_embeddings = embedding_model(text1_input_ids, text1_attention_mask)\n",
        "            text2_embeddings = embedding_model(text2_input_ids, text2_attention_mask)\n",
        "\n",
        "            # Compute similarity - dot product of normalized vectors = cosine similarity\n",
        "            similarity = torch.matmul(text1_embeddings, text2_embeddings.T)\n",
        "\n",
        "            # Create targets - diagonal elements should be 1 (positive pairs)\n",
        "            targets = torch.eye(similarity.shape[0], device=device)\n",
        "\n",
        "            # Contrastive loss - push positive pairs together, negative pairs apart\n",
        "            loss = F.binary_cross_entropy_with_logits(similarity, targets)\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        avg_loss = total_loss / len(train_loader)\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}\")\n",
        "\n",
        "    # Save model and tokenizer\n",
        "    os.makedirs(model_save_path, exist_ok=True)\n",
        "    torch.save(embedding_model.state_dict(), os.path.join(model_save_path, \"model.pt\"))\n",
        "    tokenizer.save_pretrained(model_save_path)\n",
        "\n",
        "    return embedding_model, tokenizer\n",
        "\n",
        "# === Embedding Generation ===\n",
        "def generate_embeddings(model, tokenizer, texts):\n",
        "    \"\"\"Generate embeddings for a list of texts\"\"\"\n",
        "    model.eval()\n",
        "    embeddings = []\n",
        "    batch_size = 16\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, len(texts), batch_size):\n",
        "            batch_texts = texts[i:i+batch_size]\n",
        "\n",
        "            encoded = tokenizer(\n",
        "                batch_texts,\n",
        "                padding=\"max_length\",\n",
        "                truncation=True,\n",
        "                max_length=128,\n",
        "                return_tensors=\"pt\"\n",
        "            ).to(device)\n",
        "\n",
        "            batch_embeddings = model(\n",
        "                encoded[\"input_ids\"],\n",
        "                encoded[\"attention_mask\"]\n",
        "            )\n",
        "\n",
        "            embeddings.append(batch_embeddings.cpu().numpy())\n",
        "\n",
        "    return np.vstack(embeddings)\n",
        "\n",
        "# === Game Logic ===\n",
        "class MedicalEmbeddingGame:\n",
        "    \"\"\"Game class for the medical embedding game\"\"\"\n",
        "    def __init__(self, model_path=\"./joint_embedding_model\"):\n",
        "        self.tokenizer = BertTokenizer.from_pretrained(model_path)\n",
        "\n",
        "        # Load model\n",
        "        self.model = JointEmbeddingModel().to(device)\n",
        "        self.model.load_state_dict(torch.load(os.path.join(model_path, \"model.pt\")))\n",
        "        self.model.eval()\n",
        "\n",
        "        # Load disease and symptom data\n",
        "        self.disease_embeddings = None\n",
        "        self.disease_names = None\n",
        "        self.symptom_embeddings = None\n",
        "        self.symptom_names = None\n",
        "        self.disease_to_symptoms = None\n",
        "\n",
        "        # Initialize nearest neighbor models\n",
        "        self.disease_nn = None\n",
        "        self.symptom_nn = None\n",
        "\n",
        "    def load_embeddings(self, embeddings_path=\"./embeddings\"):\n",
        "        \"\"\"Load precomputed embeddings\"\"\"\n",
        "        self.disease_embeddings = np.load(os.path.join(embeddings_path, \"disease_embeddings.npy\"))\n",
        "        self.symptom_embeddings = np.load(os.path.join(embeddings_path, \"symptom_embeddings.npy\"))\n",
        "\n",
        "        # Load names\n",
        "        self.disease_names = joblib.load(os.path.join(embeddings_path, \"disease_names.pkl\"))\n",
        "        self.symptom_names = joblib.load(os.path.join(embeddings_path, \"symptom_names.pkl\"))\n",
        "\n",
        "        # Load disease to symptoms mapping\n",
        "        self.disease_to_symptoms = joblib.load(os.path.join(embeddings_path, \"disease_to_symptoms.pkl\"))\n",
        "\n",
        "        # Initialize nearest neighbor models\n",
        "        self.disease_nn = NearestNeighbors(n_neighbors=5, metric=\"cosine\")\n",
        "        self.disease_nn.fit(self.disease_embeddings)\n",
        "\n",
        "        self.symptom_nn = NearestNeighbors(n_neighbors=5, metric=\"cosine\")\n",
        "        self.symptom_nn.fit(self.symptom_embeddings)\n",
        "\n",
        "        print(f\"Loaded {len(self.disease_names)} diseases and {len(self.symptom_names)} symptoms\")\n",
        "\n",
        "    def find_similar_diseases(self, embedding, top_k=5):\n",
        "        \"\"\"Find similar diseases given an embedding\"\"\"\n",
        "        distances, indices = self.disease_nn.kneighbors([embedding], n_neighbors=top_k)\n",
        "        return [(self.disease_names[idx], 1-dist) for dist, idx in zip(distances[0], indices[0])]\n",
        "\n",
        "    def find_similar_symptoms(self, embedding, top_k=5):\n",
        "        \"\"\"Find similar symptoms given an embedding\"\"\"\n",
        "        distances, indices = self.symptom_nn.kneighbors([embedding], n_neighbors=top_k)\n",
        "        return [(self.symptom_names[idx], 1-dist) for dist, idx in zip(distances[0], indices[0])]\n",
        "\n",
        "    def get_disease_embedding(self, disease_name):\n",
        "        \"\"\"Get embedding for a disease by name\"\"\"\n",
        "        if disease_name in self.disease_names:\n",
        "            idx = self.disease_names.index(disease_name)\n",
        "            return self.disease_embeddings[idx]\n",
        "        else:\n",
        "            # Generate embedding for unknown disease\n",
        "            return self._embed_text(disease_name)\n",
        "\n",
        "    def get_symptom_embedding(self, symptom_name):\n",
        "        \"\"\"Get embedding for a symptom by name\"\"\"\n",
        "        if symptom_name in self.symptom_names:\n",
        "            idx = self.symptom_names.index(symptom_name)\n",
        "            return self.symptom_embeddings[idx]\n",
        "        else:\n",
        "            # Generate embedding for unknown symptom\n",
        "            return self._embed_text(symptom_name)\n",
        "\n",
        "    def _embed_text(self, text):\n",
        "        \"\"\"Generate embedding for a text using the model\"\"\"\n",
        "        encoded = self.tokenizer(\n",
        "            text,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            max_length=128,\n",
        "            return_tensors=\"pt\"\n",
        "        ).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            embedding = self.model(\n",
        "                encoded[\"input_ids\"],\n",
        "                encoded[\"attention_mask\"]\n",
        "            )\n",
        "\n",
        "        return embedding.cpu().numpy()[0]\n",
        "\n",
        "    def merge_symptoms(self, symptom_names):\n",
        "        \"\"\"Merge multiple symptom embeddings\"\"\"\n",
        "        embeddings = []\n",
        "        for symptom in symptom_names:\n",
        "            embeddings.append(self.get_symptom_embedding(symptom))\n",
        "\n",
        "        # Simple average of embeddings\n",
        "        merged_embedding = np.mean(embeddings, axis=0)\n",
        "        # Normalize the merged embedding\n",
        "        merged_embedding = merged_embedding / np.linalg.norm(merged_embedding)\n",
        "\n",
        "        return merged_embedding\n",
        "\n",
        "    def predict_disease_from_symptoms(self, symptom_names):\n",
        "        \"\"\"Predict disease from symptoms\"\"\"\n",
        "        merged_embedding = self.merge_symptoms(symptom_names)\n",
        "        return self.find_similar_diseases(merged_embedding, top_k=1)[0]\n",
        "\n",
        "    def calculate_embedding_similarity(self, disease_name, symptom_names):\n",
        "        \"\"\"Calculate cosine similarity between disease and merged symptom embeddings\"\"\"\n",
        "        disease_emb = self.get_disease_embedding(disease_name)\n",
        "        merged_symptom_emb = self.merge_symptoms(symptom_names)\n",
        "\n",
        "        similarity = 1 - np.dot(disease_emb, merged_symptom_emb)\n",
        "        return similarity\n",
        "\n",
        "    def evaluate_hypothesis(self, test_samples=None):\n",
        "        \"\"\"Evaluate how well symptom embeddings match disease embeddings\"\"\"\n",
        "        if test_samples is None:\n",
        "            # Use all known disease-symptom relationships\n",
        "            test_samples = [(disease, symptoms)\n",
        "                           for disease, symptoms in self.disease_to_symptoms.items()]\n",
        "\n",
        "        accuracies = []\n",
        "        similarities = []\n",
        "\n",
        "        for disease, symptoms in test_samples:\n",
        "            # Skip entries with no symptoms\n",
        "            if not symptoms:\n",
        "                continue\n",
        "\n",
        "            # Predict disease from symptoms\n",
        "            merged_emb = self.merge_symptoms(symptoms)\n",
        "            predicted_disease, similarity = self.find_similar_diseases(merged_emb, top_k=1)[0]\n",
        "\n",
        "            # Calculate accuracy (1 if correct, 0 if wrong)\n",
        "            accuracy = 1 if predicted_disease == disease else 0\n",
        "            accuracies.append(accuracy)\n",
        "\n",
        "            # Calculate similarity\n",
        "            disease_emb = self.get_disease_embedding(disease)\n",
        "            cosine_sim = 1 - np.linalg.norm(disease_emb - merged_emb)\n",
        "            similarities.append(cosine_sim)\n",
        "\n",
        "        results = {\n",
        "            \"accuracy\": np.mean(accuracies),\n",
        "            \"avg_similarity\": np.mean(similarities),\n",
        "            \"num_samples\": len(accuracies)\n",
        "        }\n",
        "\n",
        "        return results\n",
        "\n",
        "# === Main Execution ===\n",
        "def main():\n",
        "    # Step 1: Load data\n",
        "    df = load_data()\n",
        "\n",
        "    # Step 2: Prepare training data\n",
        "    pairs, disease_encoder, symptom_encoder = prepare_training_data(df)\n",
        "    train_pairs, test_pairs = split_data(pairs)\n",
        "\n",
        "    # Step 3: Train embedding model\n",
        "    model_save_path = \"./joint_embedding_model\"\n",
        "    embedding_model, tokenizer = train_embedding_model(train_pairs, test_pairs, model_save_path)\n",
        "\n",
        "    # Step 4: Generate and save embeddings\n",
        "    print(\"Generating embeddings...\")\n",
        "\n",
        "    # Extract unique diseases and symptoms\n",
        "    symptom_cols = [col for col in df.columns if col.startswith(\"Symptom_\")]\n",
        "    disease_names = []\n",
        "    symptom_names = []\n",
        "    disease_to_symptoms = {}\n",
        "\n",
        "    for _, row in df.iterrows():\n",
        "        disease = str(row[\"Disease\"]).strip()\n",
        "        symptoms = [str(row[col]).strip() for col in symptom_cols\n",
        "                    if pd.notna(row[col]) and str(row[col]).strip().lower() != \"nan\"]\n",
        "\n",
        "        if disease and disease not in disease_names:\n",
        "            disease_names.append(disease)\n",
        "            disease_to_symptoms[disease] = symptoms\n",
        "\n",
        "        for symptom in symptoms:\n",
        "            if symptom and symptom not in symptom_names:\n",
        "                symptom_names.append(symptom)\n",
        "\n",
        "    # Generate embeddings\n",
        "    disease_embeddings = generate_embeddings(embedding_model, tokenizer, disease_names)\n",
        "    symptom_embeddings = generate_embeddings(embedding_model, tokenizer, symptom_names)\n",
        "\n",
        "    # Save embeddings and metadata\n",
        "    embeddings_path = \"./embeddings\"\n",
        "    os.makedirs(embeddings_path, exist_ok=True)\n",
        "\n",
        "    np.save(os.path.join(embeddings_path, \"disease_embeddings.npy\"), disease_embeddings)\n",
        "    np.save(os.path.join(embeddings_path, \"symptom_embeddings.npy\"), symptom_embeddings)\n",
        "\n",
        "    joblib.dump(disease_names, os.path.join(embeddings_path, \"disease_names.pkl\"))\n",
        "    joblib.dump(symptom_names, os.path.join(embeddings_path, \"symptom_names.pkl\"))\n",
        "    joblib.dump(disease_to_symptoms, os.path.join(embeddings_path, \"disease_to_symptoms.pkl\"))\n",
        "\n",
        "    # Step 5: Initialize game and test hypothesis\n",
        "    print(\"Testing hypothesis...\")\n",
        "    game = MedicalEmbeddingGame(model_save_path)\n",
        "    game.load_embeddings(embeddings_path)\n",
        "\n",
        "    # Test adding symptom embeddings\n",
        "    results = game.evaluate_hypothesis()\n",
        "    print(f\"Hypothesis results:\")\n",
        "    print(f\"Accuracy: {results['accuracy']:.4f}\")\n",
        "    print(f\"Average similarity: {results['avg_similarity']:.4f}\")\n",
        "    print(f\"Number of samples: {results['num_samples']}\")\n",
        "\n",
        "    # Step 6: Interactive demo\n",
        "    print(\"\\nDemo: Predicting disease from symptoms\")\n",
        "    sample_disease = disease_names[0]\n",
        "    sample_symptoms = disease_to_symptoms[sample_disease]\n",
        "\n",
        "    print(f\"Known disease: {sample_disease}\")\n",
        "    print(f\"Known symptoms: {sample_symptoms}\")\n",
        "\n",
        "    # Predict using merged embeddings\n",
        "    merged_emb = game.merge_symptoms(sample_symptoms)\n",
        "    predicted_disease, similarity = game.find_similar_diseases(merged_emb, top_k=1)[0]\n",
        "\n",
        "    print(f\"Predicted disease: {predicted_disease}\")\n",
        "    print(f\"Similarity score: {similarity:.4f}\")\n",
        "\n",
        "    return game\n",
        "\n",
        "# === Example Usage ===\n",
        "def example_predict_disease(game, symptoms):\n",
        "    \"\"\"Example function to predict disease from symptoms\"\"\"\n",
        "    disease, similarity = game.predict_disease_from_symptoms(symptoms)\n",
        "    print(f\"Symptoms: {symptoms}\")\n",
        "    print(f\"Predicted disease: {disease} (similarity: {similarity:.4f})\")\n",
        "    return disease\n",
        "\n",
        "def example_analyze_embedding_space(game):\n",
        "    \"\"\"Analyze the embedding space by visualizing similarities\"\"\"\n",
        "    # Select a few diseases and their symptoms for visualization\n",
        "    sample_diseases = list(game.disease_to_symptoms.keys())[:10]\n",
        "\n",
        "    # Calculate similarities between diseases and their symptoms\n",
        "    similarities = []\n",
        "    for disease in sample_diseases:\n",
        "        symptoms = game.disease_to_symptoms[disease]\n",
        "        if symptoms:\n",
        "            sim = game.calculate_embedding_similarity(disease, symptoms)\n",
        "            similarities.append((disease, sim))\n",
        "\n",
        "    # Plot similarity scores\n",
        "    diseases, scores = zip(*similarities)\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.bar(diseases, scores)\n",
        "    plt.xticks(rotation=90)\n",
        "    plt.title(\"Disease-Symptom Embedding Similarity\")\n",
        "    plt.ylabel(\"Cosine Similarity\")\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(\"disease_symptom_similarity.png\")\n",
        "    plt.close()\n",
        "\n",
        "    print(f\"Saved similarity analysis to disease_symptom_similarity.png\")\n",
        "    return similarities\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    game = main()\n",
        "\n",
        "    # Additional examples\n",
        "    print(\"\\nExample: Predicting disease from symptoms\")\n",
        "    example_predict_disease(game, [\"fever\", \"cough\", \"fatigue\"])\n",
        "\n",
        "    print(\"\\nExample: Analyzing embedding space\")\n",
        "    example_analyze_embedding_space(game)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "def zip_folder(folder_path, output_path):\n",
        "    \"\"\"Compresses the contents of a folder into a ZIP file.\n",
        "\n",
        "    Args:\n",
        "        folder_path (str): The path to the folder you want to compress.\n",
        "        output_path (str): The desired path for the output ZIP file.\n",
        "    \"\"\"\n",
        "    with zipfile.ZipFile(output_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
        "        for root, dirs, files in os.walk(folder_path):\n",
        "            for file in files:\n",
        "                file_path = os.path.join(root, file)\n",
        "                relative_path = os.path.relpath(file_path, folder_path)\n",
        "                zipf.write(file_path, relative_path)\n",
        "\n",
        "# Example usage:\n",
        "folder_to_compress = '/kaggle/working/data'  # Replace with the actual path to your folder\n",
        "output_zip_file = '/kaggle/working/data.zip' # Specify the desired output ZIP file name\n",
        "\n",
        "zip_folder(folder_to_compress, output_zip_file)\n",
        "\n",
        "print(f\"Folder '{folder_to_compress}' has been successfully compressed to '{output_zip_file}'\")"
      ],
      "metadata": {
        "id": "6v8a8p60mkgd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "import joblib\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "\n",
        "def convert_embeddings_to_json(embeddings_dir=\"./kaggle/working/data\", output_dir=\"./frontend/public/data\"):\n",
        "    \"\"\"\n",
        "    Convert saved embeddings and metadata to JSON format for use in React\n",
        "\n",
        "    Args:\n",
        "        embeddings_dir: Directory containing saved embeddings\n",
        "        output_dir: Directory to save JSON files\n",
        "    \"\"\"\n",
        "    # Create output directory if it doesn't exist\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    # Load embeddings and metadata\n",
        "    try:\n",
        "        disease_embeddings = np.load(os.path.join(embeddings_dir, \"disease_embeddings.npy\"))\n",
        "        symptom_embeddings = np.load(os.path.join(embeddings_dir, \"symptom_embeddings.npy\"))\n",
        "\n",
        "        disease_names = joblib.load(os.path.join(embeddings_dir, \"disease_names.pkl\"))\n",
        "        symptom_names = joblib.load(os.path.join(embeddings_dir, \"symptom_names.pkl\"))\n",
        "        disease_to_symptoms = joblib.load(os.path.join(embeddings_dir, \"disease_to_symptoms.pkl\"))\n",
        "\n",
        "        print(f\"Loaded {len(disease_names)} diseases and {len(symptom_names)} symptoms\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading embeddings: {e}\")\n",
        "        return\n",
        "\n",
        "    # Build nearest neighbors model for diseases\n",
        "    disease_nn = NearestNeighbors(n_neighbors=5, metric=\"cosine\")\n",
        "    disease_nn.fit(disease_embeddings)\n",
        "\n",
        "    # Create JSON structures\n",
        "\n",
        "    # 1. Disease data with embeddings\n",
        "    disease_data = []\n",
        "    for i, disease in enumerate(disease_names):\n",
        "        disease_data.append({\n",
        "            \"id\": i,\n",
        "            \"name\": disease,\n",
        "            \"embedding\": disease_embeddings[i].tolist(),\n",
        "            \"symptoms\": disease_to_symptoms.get(disease, [])\n",
        "        })\n",
        "\n",
        "    # 2. Symptom data with embeddings\n",
        "    symptom_data = []\n",
        "    for i, symptom in enumerate(symptom_names):\n",
        "        symptom_data.append({\n",
        "            \"id\": i,\n",
        "            \"name\": symptom,\n",
        "            \"embedding\": symptom_embeddings[i].tolist()\n",
        "        })\n",
        "\n",
        "    # 3. Generate some game levels (randomly select diseases)\n",
        "    import random\n",
        "\n",
        "    # Select diseases that have at least 3 symptoms\n",
        "    eligible_diseases = [d for d in disease_data if len(d[\"symptoms\"]) >= 3]\n",
        "\n",
        "    # Create 20 game levels or fewer if we don't have enough eligible diseases\n",
        "    num_levels = min(20, len(eligible_diseases))\n",
        "    game_levels = []\n",
        "\n",
        "    selected_disease_indices = random.sample(range(len(eligible_diseases)), num_levels)\n",
        "\n",
        "    for level, idx in enumerate(selected_disease_indices):\n",
        "        disease = eligible_diseases[idx]\n",
        "        game_levels.append({\n",
        "            \"level\": level + 1,\n",
        "            \"targetDisease\": disease[\"name\"],\n",
        "            \"diseaseId\": disease[\"id\"],\n",
        "            \"availableSymptoms\": disease[\"symptoms\"],\n",
        "            # Add some random symptoms as distractors\n",
        "            \"distractorSymptoms\": random.sample([s[\"name\"] for s in symptom_data\n",
        "                                                if s[\"name\"] not in disease[\"symptoms\"]],\n",
        "                                               min(5, len(symptom_data)))\n",
        "        })\n",
        "\n",
        "    # Write to JSON files\n",
        "    with open(os.path.join(output_dir, \"diseases.json\"), \"w\") as f:\n",
        "        json.dump(disease_data, f)\n",
        "\n",
        "    with open(os.path.join(output_dir, \"symptoms.json\"), \"w\") as f:\n",
        "        json.dump(symptom_data, f)\n",
        "\n",
        "    with open(os.path.join(output_dir, \"game_levels.json\"), \"w\") as f:\n",
        "        json.dump(game_levels, f)\n",
        "\n",
        "    # Create a smaller metadata file without embeddings for faster loading\n",
        "    disease_metadata = [{\"id\": d[\"id\"], \"name\": d[\"name\"], \"symptoms\": d[\"symptoms\"]}\n",
        "                        for d in disease_data]\n",
        "\n",
        "    symptom_metadata = [{\"id\": s[\"id\"], \"name\": s[\"name\"]} for s in symptom_data]\n",
        "\n",
        "    metadata = {\n",
        "        \"diseases\": disease_metadata,\n",
        "        \"symptoms\": symptom_metadata,\n",
        "        \"levels\": game_levels\n",
        "    }\n",
        "\n",
        "    with open(os.path.join(output_dir, \"metadata.json\"), \"w\") as f:\n",
        "        json.dump(metadata, f)\n",
        "\n",
        "    print(f\"Successfully converted embeddings to JSON. Files saved to {output_dir}\")\n",
        "    print(f\"Generated {len(game_levels)} game levels\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    convert_embeddings_to_json()"
      ],
      "metadata": {
        "id": "i8PEADDHmn7V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import joblib\n",
        "import json\n",
        "import os\n",
        "\n",
        "def convert_embeddings_to_json(embeddings_path=\"/kaggle/input/embeddings/\", output_path=\"./data\"):\n",
        "    \"\"\"\n",
        "    Convert saved embeddings and metadata to JSON format for web app\n",
        "    \"\"\"\n",
        "    # Create output directory if it doesn't exist\n",
        "    os.makedirs(output_path, exist_ok=True)\n",
        "\n",
        "    # Load disease and symptom data\n",
        "    try:\n",
        "        disease_embeddings = np.load(os.path.join(embeddings_path, \"disease_embeddings.npy\"))\n",
        "        symptom_embeddings = np.load(os.path.join(embeddings_path, \"symptom_embeddings.npy\"))\n",
        "        disease_names = joblib.load(os.path.join(embeddings_path, \"disease_names.pkl\"))\n",
        "        symptom_names = joblib.load(os.path.join(embeddings_path, \"symptom_names.pkl\"))\n",
        "        disease_to_symptoms = joblib.load(os.path.join(embeddings_path, \"disease_to_symptoms.pkl\"))\n",
        "\n",
        "        print(f\"Loaded {len(disease_names)} diseases and {len(symptom_names)} symptoms\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading embeddings: {e}\")\n",
        "        return False\n",
        "\n",
        "    # Convert embeddings to lists for JSON serialization\n",
        "    disease_embeddings_list = disease_embeddings.tolist()\n",
        "    symptom_embeddings_list = symptom_embeddings.tolist()\n",
        "\n",
        "    # Create disease data with embeddings\n",
        "    disease_data = {}\n",
        "    for i, disease in enumerate(disease_names):\n",
        "        disease_data[disease] = {\n",
        "            \"embedding\": disease_embeddings_list[i],\n",
        "            \"symptoms\": disease_to_symptoms.get(disease, [])\n",
        "        }\n",
        "\n",
        "    # Create symptom data with embeddings\n",
        "    symptom_data = {}\n",
        "    for i, symptom in enumerate(symptom_names):\n",
        "        symptom_data[symptom] = {\n",
        "            \"embedding\": symptom_embeddings_list[i]\n",
        "        }\n",
        "\n",
        "    # Save data to JSON files\n",
        "    try:\n",
        "        with open(os.path.join(output_path, \"diseases.json\"), \"w\") as f:\n",
        "            json.dump(disease_data, f)\n",
        "\n",
        "        with open(os.path.join(output_path, \"symptoms.json\"), \"w\") as f:\n",
        "            json.dump(symptom_data, f)\n",
        "\n",
        "        # Create a simplified symptom list (without embeddings) for the UI\n",
        "        symptom_list = list(symptom_names)\n",
        "        with open(os.path.join(output_path, \"symptom_list.json\"), \"w\") as f:\n",
        "            json.dump(symptom_list, f)\n",
        "\n",
        "        # Create a simplified disease list (without embeddings) for the UI\n",
        "        disease_list = list(disease_names)\n",
        "        with open(os.path.join(output_path, \"disease_list.json\"), \"w\") as f:\n",
        "            json.dump(disease_list, f)\n",
        "\n",
        "        print(f\"Successfully saved JSON data to {output_path}\")\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"Error saving JSON: {e}\")\n",
        "        return False\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Convert embeddings to JSON\n",
        "    convert_embeddings_to_json()"
      ],
      "metadata": {
        "id": "SsZwrP-QmtHq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}